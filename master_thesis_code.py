{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30d089-40ee-4482-b887-16e0baae88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import dask\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "\n",
    "import time\n",
    "\n",
    "# Misura il tempo di esecuzione\n",
    "start_time = time.time()\n",
    "\n",
    "# Parametri della distribuzione Gamma\n",
    "media = 1\n",
    "varianza = 4\n",
    "k = media ** 2 / varianza  # Parametro forma\n",
    "theta = varianza / media   # Parametro scala\n",
    "\n",
    "# Genera 1000000 valori casuali dalla distribuzione Gamma\n",
    "random_values = gamma.rvs(k, scale=theta, size=1000000)\n",
    "\n",
    "# Calcola il 99-esimo percentile dei valori generati\n",
    "percentile_99_5 = np.percentile(random_values, 99.5)\n",
    "\n",
    "# Dati\n",
    "rating = ['A', 'BBB', 'BB', 'B', 'CCC']  # Vettore dei rating (colonna)\n",
    "p = np.array([0.06, 0.20, 1.25, 6.25, 17.50])  # Probabilità di default media (colonna)\n",
    "w = np.array([1.011, 0.836, 0.602, 0.415, 0.295])  # Peso relativo del rischio sistematico (colonna)\n",
    "\n",
    "# Creazione della tabella con pandas\n",
    "rating_table = pd.DataFrame({\n",
    "    'Rating': rating,\n",
    "    'p_mean': p,\n",
    "    'w': w\n",
    "})\n",
    "\n",
    "# Visualizzazione della tabella\n",
    "print('Tabella dei valori associati:')\n",
    "print(rating_table)\n",
    "\n",
    "E_LGD = 0.50  # Valore atteso della LGD\n",
    "\n",
    "# Calcolo di mu_x per ogni riga della tabella\n",
    "rating_table['mu_x'] = E_LGD * rating_table['p_mean'] * (1 + rating_table['w'] * (percentile_99_5 - 1))\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print('Tabella con i valori di mu_x:')\n",
    "print(rating_table)\n",
    "\n",
    "sigma = 2\n",
    "lambda_ = 0.5\n",
    "eta = 0.25\n",
    "sigma2 = sigma ** 2\n",
    "\n",
    "# Calcolo di Beta per ogni riga della tabella\n",
    "rating_table['Beta'] = (1 / (2 * lambda_)) * (lambda_ ** 2 + eta ** 2) * (\n",
    "        (1 / sigma2) * (1 + (sigma2 - 1) / percentile_99_5) *\n",
    "        (percentile_99_5 + (1 - rating_table['w']) / rating_table['w']) - 1)\n",
    "\n",
    "# Valori di n e nomi delle colonne\n",
    "n_values = [5000, 2000, 1000, 500, 200, 100]\n",
    "column_names = ['VaR5000', 'VaR2000', 'VaR1000', 'VaR500', 'VaR200', 'VaR100']\n",
    "\n",
    "# Compilazione delle colonne con mu_x + (Beta / n) * 100\n",
    "for i, n in enumerate(n_values):\n",
    "    rating_table[column_names[i]] = rating_table['mu_x'] + (rating_table['Beta'] / n) * 100\n",
    "\n",
    "# Rimozione della colonna Beta dalla tabella\n",
    "rating_table = rating_table.drop(columns=['Beta'])\n",
    "\n",
    "# Salvataggio in formato Excel\n",
    "rating_table.to_excel('/users/lorenzovancadsand/downloads/granularity_add-on.xlsx', index=False)\n",
    "\n",
    "# Parallel computing - Eseguiamo i calcoli in parallelo utilizzando Dask\n",
    "# Usiamo Dask per calcolare il 99° percentile e sommare\n",
    "dask_data = da.from_array(random_values, chunks=100000)  # Creiamo un array Dask per la distribuzione Gamma\n",
    "\n",
    "@delayed\n",
    "def compute_percentile(data):\n",
    "    return np.percentile(data, 99.5)\n",
    "\n",
    "# Eseguiamo il calcolo in parallelo\n",
    "result = compute_percentile(dask_data)\n",
    "\n",
    "# Eseguiamo la computazione e otteniamo il risultato\n",
    "percentile_99_5_parallel = result.compute()\n",
    "print(f'99° Percentile calcolato in parallelo: {percentile_99_5_parallel}')\n",
    "\n",
    "# Lettura del file CSV\n",
    "file_path = '/Users/lorenzovancadsand/Downloads/portafoglio_Asia.csv'\n",
    "portafoglio = pd.read_csv(file_path)\n",
    "\n",
    "# Visualizzazione della tabella letta\n",
    "print('Portafoglio simulazioni:')\n",
    "print(portafoglio)\n",
    "\n",
    "# Parametri per la distribuzione Gamma\n",
    "lambda_ = 0.5  # Media\n",
    "eta = 0.25     # Deviazione standard\n",
    "\n",
    "# Recupera il numero di titoli dalla colonna 'Titolo'\n",
    "num_titoli = len(portafoglio)\n",
    "\n",
    "# Simulazione dei valori di LGD_j per ogni titolo (un solo valore per titolo)\n",
    "LGD_j = np.random.gamma(shape=lambda_, scale=eta, size=num_titoli)\n",
    "\n",
    "# Aggiungi LGD_j alla tabella 'portafoglio'\n",
    "portafoglio['LGD_j'] = LGD_j\n",
    "portafoglio = portafoglio.round(3)\n",
    "\n",
    "# Definizione delle regioni e percentuali associate\n",
    "regioni = ['NORTH_AM', 'EUROPE', 'ASIA', 'AFRICA', 'LATIN_AM', 'REST_WORLD']\n",
    "regioni_associate = portafoglio['Regione']  # Colonna delle regioni associate ai titoli\n",
    "\n",
    "# Definizione degli anni (senza la \"x\")\n",
    "years = [2025, 2030, 2035, 2040, 2045, 2050]\n",
    "\n",
    "# Visualizzazione della tabella aggiornata\n",
    "print('Portafoglio aggiornato con LGD_j:')\n",
    "print(portafoglio)\n",
    "\n",
    "# Caricamento filtrato degli shock per la combinazione selezionata\n",
    "filename_shock = '/Users/lorenzovancadsand/Downloads/Shock_Asia(Foglio1).csv'\n",
    "shock_data = pd.read_csv(filename_shock)\n",
    "\n",
    "# Visualizza le prime righe per verifica\n",
    "print(shock_data.head())\n",
    "\n",
    "# Approssima tutte le colonne numeriche al terzo decimale\n",
    "shock_data = shock_data.round(3)\n",
    "\n",
    "# Visualizza le prime righe della tabella per verificare\n",
    "print(shock_data.head())\n",
    "\n",
    "# Raggruppa e calcola il massimo degli shock per ogni combinazione di REGION, MODEL, SCENARIO\n",
    "max_shock_by_region = shock_data.groupby(['REGION', 'MODEL', 'SCENARIO'], as_index=False).agg({\n",
    "    '2025': 'max',\n",
    "    '2030': 'max',\n",
    "    '2035': 'max',\n",
    "    '2040': 'max',\n",
    "    '2045': 'max',\n",
    "    '2050': 'max'\n",
    "})\n",
    "\n",
    "# Rinomina le colonne per chiarezza\n",
    "max_shock_by_region.rename(columns={\n",
    "    '2025': 'Max_Shock_2025',\n",
    "    '2030': 'Max_Shock_2030',\n",
    "    '2035': 'Max_Shock_2035',\n",
    "    '2040': 'Max_Shock_2040',\n",
    "    '2045': 'Max_Shock_2045',\n",
    "    '2050': 'Max_Shock_2050'\n",
    "}, inplace=True)\n",
    "\n",
    "# Mostra i risultati per verifica (prime 10 righe)\n",
    "print(max_shock_by_region.head(10))\n",
    "\n",
    "# Visualizza le prime righe per verifica\n",
    "print(shock_data)\n",
    "\n",
    "# Definizione dei settori in un dizionario\n",
    "settori_data = {\n",
    "    'primary_fossil': [\n",
    "        'Primary Energy|Coal',\n",
    "        'Primary Energy|Gas',\n",
    "        'Primary Energy|Oil'\n",
    "    ],\n",
    "    'secondary_fossil': [\n",
    "        'Secondary Energy|Electricity|Coal',\n",
    "        'Secondary Energy|Electricity|Gas',\n",
    "        'Secondary Energy|Electricity|Oil'\n",
    "    ],\n",
    "    'secondary_renewable': [\n",
    "        'Secondary Energy|Electricity|Biomass',\n",
    "        'Secondary Energy|Electricity|Geothermal',\n",
    "        'Secondary Energy|Electricity|Solar|CSP',\n",
    "        'Secondary Energy|Electricity|Nuclear',\n",
    "        'Secondary Energy|Electricity|Solar|PV',\n",
    "        'Secondary Energy|Electricity|Wind',\n",
    "        'Secondary Energy|Electricity|Hydro'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Accesso ai dati\n",
    "print(\"Settori Primary Fossil:\")\n",
    "print(settori_data['primary_fossil'])\n",
    "\n",
    "print(\"\\nSettori Secondary Renewable:\")\n",
    "print(settori_data['secondary_renewable'])\n",
    "\n",
    "# Definizione dei settori\n",
    "settori_data = {\n",
    "    'primary_fossil': ['Primary Energy|Coal', 'Primary Energy|Gas', 'Primary Energy|Oil'],\n",
    "    'secondary_fossil': ['Secondary Energy|Electricity|Coal', 'Secondary Energy|Electricity|Gas', 'Secondary Energy|Electricity|Oil'],\n",
    "    'secondary_renewable': ['Secondary Energy|Electricity|Biomass', 'Secondary Energy|Electricity|Geothermal',\n",
    "                            'Secondary Energy|Electricity|Solar|CSP', 'Secondary Energy|Electricity|Nuclear',\n",
    "                            'Secondary Energy|Electricity|Solar|PV', 'Secondary Energy|Electricity|Wind', 'Secondary Energy|Electricity|Hydro']\n",
    "}\n",
    "\n",
    "#Definizione della combinazione modello/scenario su cui lavorare\n",
    "model = 'REMIND'\n",
    "scenario = 'LIMITSOilIndependence'\n",
    "\n",
    "# Filtra solo le righe della combinazione selezionata\n",
    "shock_data = shock_data[(shock_data['MODEL'] == model) & (shock_data['SCENARIO'] == scenario)]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Parametri\n",
    "num_simulazioni = 10 # Numero di simulazioni\n",
    "max_sottosettori_per_settore = 3  # Numero massimo di sotto-settori per settore\n",
    "\n",
    "# Definizione dei settori\n",
    "settori_data = {\n",
    "    'primary_fossil': ['Primary Energy|Coal', 'Primary Energy|Gas', 'Primary Energy|Oil'],\n",
    "    'secondary_fossil': ['Secondary Energy|Electricity|Coal', 'Secondary Energy|Electricity|Gas', 'Secondary Energy|Electricity|Oil'],\n",
    "    'secondary_renewable': ['Secondary Energy|Electricity|Biomass', 'Secondary Energy|Electricity|Geothermal',\n",
    "                            'Secondary Energy|Electricity|Solar|CSP', 'Secondary Energy|Electricity|Nuclear',\n",
    "                            'Secondary Energy|Electricity|Solar|PV', 'Secondary Energy|Electricity|Wind', 'Secondary Energy|Electricity|Hydro']\n",
    "}\n",
    "\n",
    "# Percentuali massime per settore\n",
    "max_percentuali = {\n",
    "    'primary_fossil': 0,\n",
    "    'secondary_fossil': 0,\n",
    "    'secondary_renewable': 100\n",
    "}\n",
    "\n",
    "# Step 0: Rinomina la colonna 'Regione' in 'REGION' per uniformità\n",
    "portafoglio.rename(columns={'Regione': 'REGION'}, inplace=True)\n",
    "\n",
    "# Step 0: Creazione di un dizionario per LGD_j per accesso rapido\n",
    "lgd_lookup = portafoglio.set_index('Titolo')['LGD_j'].to_dict()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Funzione da parallelizzare\n",
    "def process_simulation(sim):\n",
    "    risultati_locali = []  # Lista locale per i risultati\n",
    "    for _, row in portafoglio.iterrows():\n",
    "        titolo = row['Titolo']\n",
    "        regione = row['REGION']\n",
    "        \n",
    "        settori_titolo = []  # Lista per i settori selezionati\n",
    "        \n",
    "        # Itera sui settori principali e seleziona sotto-settori\n",
    "        for settore, percentuale in max_percentuali.items():\n",
    "            if percentuale > 0:\n",
    "                available_settori = settori_data[settore]\n",
    "                selezione_settori = random.sample(available_settori, min(max_sottosettori_per_settore, len(available_settori)))\n",
    "                settori_titolo.extend(selezione_settori)\n",
    "        \n",
    "        lgd_j = lgd_lookup.get(titolo, None)\n",
    "        \n",
    "        # Salva i risultati preliminari con LGD_j\n",
    "        for settore in settori_titolo:\n",
    "            risultati_locali.append({\n",
    "                'num_simulazione': sim,\n",
    "                'Titolo': titolo,\n",
    "                'REGION': regione,\n",
    "                'settore': settore,\n",
    "                'LGD_j': lgd_j,\n",
    "            })\n",
    "    return risultati_locali\n",
    "\n",
    "# Parallelizza il ciclo esterno sulle simulazioni\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_simulation)(sim) for sim in tqdm(range(1, num_simulazioni + 1), desc=\"Processing simulations\")\n",
    ")\n",
    "\n",
    "# Concatenazione dei risultati\n",
    "risultati_df = pd.DataFrame([item for sublist in results for item in sublist])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def genera_pesi_interi(gruppo):\n",
    "    \"\"\"\n",
    "    Genera pesi come numeri interi normalizzati con somma esattamente pari a 100.\n",
    "\n",
    "    Parametri: \n",
    "    - gruppo (DataFrame): Gruppo di righe a cui associare i pesi.\n",
    "\n",
    "    Ritorna:\n",
    "    - gruppo (DataFrame): Gruppo con una nuova colonna 'peso' contenente i pesi interi.\n",
    "    \"\"\"\n",
    "    num_righe = len(gruppo)\n",
    "    pesi = np.random.randint(1, 100, size=num_righe)  # Genera numeri interi positivi casuali\n",
    "    \n",
    "    # Normalizza e arrotonda i pesi a interi\n",
    "    pesi = np.round(pesi / pesi.sum() * 100).astype(int)\n",
    "    \n",
    "    # Aggiustamento finale per garantire la somma a 100\n",
    "    differenza = 100 - np.sum(pesi)\n",
    "    pesi[0] += differenza  # Corregge la differenza nel primo elemento\n",
    "    \n",
    "    # Assicura che i pesi siano non negativi\n",
    "    pesi = np.clip(pesi, 0, None)  # Assicura che non ci siano pesi negativi\n",
    "    \n",
    "    gruppo['peso'] = pesi\n",
    "    return gruppo\n",
    "\n",
    "# Applica la funzione di generazione pesi\n",
    "risultati_df = risultati_df.groupby(['num_simulazione', 'Titolo']).apply(genera_pesi_interi).reset_index(drop=True)\n",
    "\n",
    "print(risultati_df)\n",
    "\n",
    "#aggiunta nuovo step\n",
    "\n",
    "from tqdm import tqdm  # Libreria per la progress bar\n",
    "\n",
    "# Definizione degli anni di riferimento\n",
    "anni = list(range(2025, 2055, 5))  # Anni da 2025 a 2050 con step di 5 anni\n",
    "\n",
    "# Creazione di una lista per accumulare le righe della nuova tabella\n",
    "nuova_tabella = []\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Funzione per processare una singola riga di risultati_df\n",
    "def process_row(row, shock_data):\n",
    "    sim = row['num_simulazione']\n",
    "    titolo = row['Titolo']\n",
    "    regione = row['REGION']\n",
    "    settore = row['settore']\n",
    "    peso = row['peso']\n",
    "    lgd_j = row['LGD_j']\n",
    "    \n",
    "    # Filtra shock_data per REGION e VARIABLE corrispondenti\n",
    "    shock_filtro = shock_data[(shock_data['REGION'] == regione) & (shock_data['VARIABLE'] == settore)]\n",
    "    \n",
    "    # Verifica se ci sono dati corrispondenti\n",
    "    if not shock_filtro.empty:\n",
    "        # Creazione di un dizionario per la nuova riga\n",
    "        nuova_riga_list = []\n",
    "        for _, shock_row in shock_filtro.iterrows():\n",
    "            modello = shock_row['MODEL']\n",
    "            scenario = shock_row['SCENARIO']\n",
    "            \n",
    "            # Aggiungi gli shock ponderati per ciascun anno\n",
    "            nuova_riga = {\n",
    "                'num_simulazione': sim,\n",
    "                'Titolo': titolo,\n",
    "                'REGION': regione,\n",
    "                'settore': settore,\n",
    "                'model': modello,\n",
    "                'scenario': scenario,\n",
    "                'peso': peso,\n",
    "                'LGD_j': lgd_j\n",
    "            }\n",
    "            \n",
    "            # Calcola gli shock ponderati per ciascun anno\n",
    "            for anno in range(2025, 2055, 5):\n",
    "                shock_value = shock_row[str(anno)]\n",
    "                nuova_riga[f'shock_{anno}'] = (shock_value * peso) / 100\n",
    "            \n",
    "            nuova_riga_list.append(nuova_riga)\n",
    "        return nuova_riga_list\n",
    "    return []\n",
    "\n",
    "# Parallelizzazione del ciclo con Joblib\n",
    "num_cores = -1  # Usa tutti i core disponibili\n",
    "results = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "    delayed(process_row)(row, shock_data) for _, row in tqdm(risultati_df.iterrows(), total=len(risultati_df), desc=\"Parallel processing\")\n",
    ")\n",
    "\n",
    "# Appiattisci la lista di liste dei risultati\n",
    "nuova_tabella = [riga for sublist in results for riga in sublist]\n",
    "\n",
    "# Creazione della nuova tabella come DataFrame\n",
    "nuova_tabella_df = pd.DataFrame(nuova_tabella)\n",
    "\n",
    "# Riordina le colonne includendo sia gli Shock che i max_shock\n",
    "colonne_ordinate = ['num_simulazione', 'Titolo', 'REGION', 'settore', 'model', 'scenario', 'peso'] + \\\n",
    "                   [f'shock_{anno}' for anno in range(2025, 2055, 5)]\n",
    "\n",
    "nuova_tabella_df = nuova_tabella_df[colonne_ordinate]\n",
    "\n",
    "# Mostra le prime righe della nuova tabella\n",
    "print(nuova_tabella_df.head())\n",
    "\n",
    "del nuova_tabella\n",
    "del risultati_df\n",
    "gc.collect() \n",
    "\n",
    "#nuovo step \n",
    "\n",
    "# Crea una copia della tabella originale per sicurezza\n",
    "tabella_ponderata = nuova_tabella_df.copy()\n",
    "\n",
    "# Calcola gli shock ponderati per ciascun anno (2025 - 2050)\n",
    "for anno in range(2025, 2055, 5):\n",
    "    colonna_shock = f'shock_{anno}'  # Colonna degli shock originali\n",
    "    colonna_ponderata = f'shock_ponderato_{anno}'  # Nuova colonna ponderata\n",
    "    \n",
    "    # Calcola lo shock ponderato\n",
    "    tabella_ponderata[colonna_ponderata] = (tabella_ponderata[colonna_shock] * tabella_ponderata['peso']) / 100\n",
    "\n",
    "# Seleziona solo le colonne richieste nella tabella finale\n",
    "colonne_finali = ['num_simulazione', 'Titolo', 'REGION', 'settore', 'model', 'scenario'] + \\\n",
    "                 [f'shock_ponderato_{anno}' for anno in range(2025, 2055, 5)] \n",
    "tabella_ponderata = tabella_ponderata[colonne_finali]\n",
    "\n",
    "# Mostra le prime righe della tabella finale\n",
    "print(tabella_ponderata.head())\n",
    "\n",
    "del nuova_tabella_df \n",
    "gc.collect() \n",
    "\n",
    "# Definizione degli anni di riferimento\n",
    "anni = range(2025, 2055, 5)\n",
    "\n",
    "# Crea una lista con i nomi delle colonne da sommare\n",
    "colonne_shock_ponderato = [f'shock_ponderato_{anno}' for anno in anni]\n",
    "\n",
    "# Raggruppa per num_simulazione, Titolo, model, scenario e REGION e somma i valori degli shock ponderati\n",
    "shock_totali = tabella_ponderata.groupby(\n",
    "    ['num_simulazione', 'Titolo', 'model', 'scenario', 'REGION'],\n",
    "    as_index=False\n",
    ")[colonne_shock_ponderato].sum()\n",
    "\n",
    "# Rinomina le colonne shock_ponderato in shock_tot_anno\n",
    "shock_totali.rename(columns={f'shock_ponderato_{anno}': f'shock_tot_{anno}' for anno in anni}, inplace=True)\n",
    "\n",
    "# Unisci i valori di max_shock_anno da max_shock_by_region\n",
    "shock_totali = shock_totali.merge(\n",
    "    max_shock_by_region[['REGION', 'MODEL', 'SCENARIO'] + [f'Max_Shock_{anno}' for anno in anni]],\n",
    "    left_on=['REGION', 'model', 'scenario'],\n",
    "    right_on=['REGION', 'MODEL', 'SCENARIO'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rinomina le colonne Max_Shock_anno in max_shock_anno per consistenza\n",
    "shock_totali.rename(columns={f'Max_Shock_{anno}': f'max_shock_{anno}' for anno in anni}, inplace=True)\n",
    "\n",
    "# Rimuovi le colonne MODEL e SCENARIO duplicate dopo il merge\n",
    "shock_totali.drop(columns=['MODEL', 'SCENARIO'], inplace=True)\n",
    "\n",
    "# Mostra le prime righe della tabella finale\n",
    "print(shock_totali.head())\n",
    "\n",
    "# Unione di LGD_j da portafoglio in shock_totali sulla base di Titolo\n",
    "shock_totali = shock_totali.merge(\n",
    "    portafoglio[['Titolo', 'LGD_j']],  # Prendi solo Titolo e LGD_j da portafoglio\n",
    "    on='Titolo',                       # Chiave di unione\n",
    "    how='left'                         # Mantieni tutte le righe di shock_totali\n",
    ")\n",
    "\n",
    "# Mostra le prime righe della tabella aggiornata\n",
    "print(shock_totali.head())\n",
    "\n",
    "# Verifica le dimensioni della tabella\n",
    "print(\"Dimensioni di shock_totali dopo il merge:\", shock_totali.shape)\n",
    "\n",
    "del tabella_ponderata \n",
    "gc.collect() \n",
    "\n",
    "# Definizione degli anni di riferimento\n",
    "anni = range(2025, 2055, 5)\n",
    "\n",
    "# Creazione di una lista per accumulare i risultati\n",
    "risultati_delta_v = []\n",
    "\n",
    "# Iterazione sulle righe di shock_totali per calcolare delta_v_anno per ciascun anno\n",
    "for _, row in shock_totali.iterrows():\n",
    "    nuova_riga = {\n",
    "        'num_simulazione': row['num_simulazione'],\n",
    "        'num_titolo': row['Titolo'],\n",
    "        'model': row['model'],\n",
    "        'scenario': row['scenario']\n",
    "    }\n",
    "    \n",
    "    # Calcola delta_v per ciascun anno\n",
    "    for anno in anni:\n",
    "        nuova_riga[f'delta_v_{anno}'] = - (row['LGD_j'] * row[f'shock_tot_{anno}']) / (row[f'max_shock_{anno}'] + 1)\n",
    "    \n",
    "    # Aggiungi la riga alla lista dei risultati\n",
    "    risultati_delta_v.append(nuova_riga)\n",
    "\n",
    "# Creazione della nuova tabella dai risultati accumulati\n",
    "delta_v_df = pd.DataFrame(risultati_delta_v)\n",
    "\n",
    "# Visualizzazione delle prime righe della tabella finale\n",
    "print(delta_v_df.head())\n",
    "\n",
    "# Definizione degli anni\n",
    "anni = range(2025, 2055, 5)\n",
    "delta_v_cols = [f'delta_v_{anno}' for anno in anni]  # Colonne da sommare\n",
    "\n",
    "# Aggregazione: somma di delta_v per combinazione di Simulazione, Model, Scenario\n",
    "aggregated_losses = delta_v_df.groupby(['num_simulazione', 'model', 'scenario'], as_index=False)[delta_v_cols].sum()\n",
    "\n",
    "# Rinomina le colonne sommate per maggiore chiarezza\n",
    "rename_dict = {f'delta_v_{anno}': f'sum_delta_v_{anno}' for anno in anni}\n",
    "aggregated_losses.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Mostra le prime righe della tabella aggregata\n",
    "print(aggregated_losses.head())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Definizione degli anni e delle colonne delle perdite\n",
    "anni = range(2025, 2055, 5)\n",
    "loss_cols = [f'sum_delta_v_{anno}' for anno in anni]  # Colonne delle perdite\n",
    "\n",
    "# Inizializziamo una lista per accumulare i risultati dei percentili\n",
    "percentile_results_list = []\n",
    "\n",
    "# Calcoliamo il 99,5° percentile per ogni anno\n",
    "for anno, loss_col in zip(anni, loss_cols):\n",
    "    percentile_col = f'percentile_99.5_{anno}'  # Nome della nuova colonna per il percentile\n",
    "    \n",
    "    if loss_col in aggregated_losses.columns:\n",
    "        # Calcolo del 99.5° percentile raggruppando per Model e Scenario\n",
    "        temp_percentile = aggregated_losses.groupby(['model', 'scenario'], as_index=False)[loss_col].quantile(0.995)\n",
    "        \n",
    "        # Rinominiamo la colonna del risultato\n",
    "        temp_percentile.rename(columns={loss_col: percentile_col}, inplace=True)\n",
    "        \n",
    "        # Accumula i risultati\n",
    "        percentile_results_list.append(temp_percentile)\n",
    "    else:\n",
    "        print(f\"Colonna {loss_col} non trovata in aggregated_losses.\")\n",
    "\n",
    "# Unione dei risultati per ciascun anno\n",
    "percentile_results = percentile_results_list[0]\n",
    "for i in range(1, len(percentile_results_list)):\n",
    "    percentile_results = pd.merge(percentile_results, percentile_results_list[i], on=['model', 'scenario'], how='inner')\n",
    "\n",
    "# Mostra i risultati\n",
    "print(percentile_results)\n",
    "\n",
    "# Definizione degli anni e del tasso risk-free\n",
    "anni = range(2025, 2055, 5)\n",
    "y_f = 0.03  # Tasso risk-free\n",
    "\n",
    "# Creiamo una copia della tabella percentile_results per i risultati aggiustati\n",
    "adjusted_table = percentile_results.copy()\n",
    "\n",
    "# Calcolo e applicazione del fattore di sconto per ogni anno\n",
    "for anno in anni:\n",
    "    T = anno - 2025  # Tempo rispetto al 2025\n",
    "    discount = np.exp(-y_f * T)  # Fattore di sconto\n",
    "    \n",
    "    # Nomi delle colonne percentile e adjusted\n",
    "    percentile_col = f'percentile_99.5_{anno}'\n",
    "    adjusted_col = f'adjusted_{anno}'\n",
    "    \n",
    "    # Applicazione del fattore di sconto se la colonna esiste\n",
    "    if percentile_col in adjusted_table.columns:\n",
    "        adjusted_table[adjusted_col] = adjusted_table[percentile_col] * discount\n",
    "    else:\n",
    "        print(f\"Colonna {percentile_col} non trovata in adjusted_table.\")\n",
    "\n",
    "# Rimuovi le colonne percentile non necessarie\n",
    "columns_to_remove = [f'percentile_99.5_{anno}' for anno in anni]\n",
    "adjusted_table.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
    "n\n",
    "# Mostra le prime righe della tabella aggiornata\n",
    "print(adjusted_table.head())\n",
    "\n",
    "output_path = '/users/lorenzovancadsand/downloads/Basis_Africa_ROIND.xlsx'\n",
    "adjusted_table.to_excel(output_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tempo totale di esecuzione: {end_time - start_time:.2f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7608f9c-23be-4617-b3be-606e2fff2c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
