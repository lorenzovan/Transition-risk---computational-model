{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30d089-40ee-4482-b887-16e0baae88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import dask\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Credit risk side of the model from Gordy\n",
    "mean = 1\n",
    "variance = 4\n",
    "k = mean ** 2 / variance \n",
    "theta = variance / mean \n",
    "\n",
    "random_values = gamma.rvs(k, scale=theta, size=1000000)\n",
    "\n",
    "percentile_99_5 = np.percentile(random_values, 99.5)\n",
    "\n",
    "rating = ['A', 'BBB', 'BB', 'B', 'CCC'] \n",
    "p = np.array([0.06, 0.20, 1.25, 6.25, 17.50])  # ProbabilitÃ  di default media (colonna)\n",
    "w = np.array([1.011, 0.836, 0.602, 0.415, 0.295])  # Peso relativo del rischio sistematico (colonna)\n",
    "\n",
    "rating_table = pd.DataFrame({\n",
    "    'Rating': rating,\n",
    "    'p_mean': p,\n",
    "    'w': w\n",
    "})\n",
    "\n",
    "print('Rating table:')\n",
    "print(rating_table)\n",
    "\n",
    "E_LGD = 0.50 \n",
    "\n",
    "rating_table['mu_x'] = E_LGD * rating_table['p_mean'] * (1 + rating_table['w'] * (percentile_99_5 - 1))\n",
    "\n",
    "print('rating table with mu_x:')\n",
    "print(rating_table)\n",
    "\n",
    "sigma = 2\n",
    "lambda_ = 0.5\n",
    "eta = 0.25\n",
    "sigma2 = sigma ** 2\n",
    "\n",
    "rating_table['Beta'] = (1 / (2 * lambda_)) * (lambda_ ** 2 + eta ** 2) * (\n",
    "        (1 / sigma2) * (1 + (sigma2 - 1) / percentile_99_5) *\n",
    "        (percentile_99_5 + (1 - rating_table['w']) / rating_table['w']) - 1)\n",
    "\n",
    "n_values = [5000, 2000, 1000, 500, 200, 100]\n",
    "column_names = ['VaR5000', 'VaR2000', 'VaR1000', 'VaR500', 'VaR200', 'VaR100']\n",
    "\n",
    "for i, n in enumerate(n_values):\n",
    "    rating_table[column_names[i]] = rating_table['mu_x'] + (rating_table['Beta'] / n) * 100\n",
    "\n",
    "rating_table = rating_table.drop(columns=['Beta'])\n",
    "\n",
    "rating_table.to_excel('/users/lorenzovancadsand/downloads/granularity_add-on.xlsx', index=False)\n",
    "\n",
    "# Parallel computing by Dask\n",
    "dask_data = da.from_array(random_values, chunks=100000)  # Creiamo un array Dask per la distribuzione Gamma\n",
    "\n",
    "@delayed\n",
    "def compute_percentile(data):\n",
    "    return np.percentile(data, 99.5)\n",
    "\n",,
    "result = compute_percentile(dask_data)\n",
    "\n",
    "percentile_99_5_parallel = result.compute()\n",
    "\n",
    "file_path = '/Users/lorenzovancadsand/Downloads/portafogliosimulazioni copia.csv'\n",
    "portafoglio = pd.read_csv(file_path)\n",
    "\n",
    "print('Portafoglio simulazioni:')\n",
    "print(portafoglio)\n",
    "\n",
    "lambda_ = 0.5  \n",
    "eta = 0.25     \n",
    "\n",
    "num_titoli = len(portafoglio)\n",
    "\n",
    "LGD_j = np.random.gamma(shape=lambda_, scale=eta, size=num_titoli)\n",
    "\n",
    "portafoglio['LGD_j'] = LGD_j\n",
    "portafoglio = portafoglio.round(3)\n",
    "\n",
    "regioni = ['NORTH_AM', 'EUROPE', 'ASIA', 'AFRICA', 'LATIN_AM', 'REST_WORLD']\n",
    "regioni_associate = portafoglio['Regione'] \n",
    "\n",
    "years = [2025, 2030, 2035, 2040, 2045, 2050]\n",
    "\n",
    "print('Portafoglio aggiornato con LGD_j:')\n",
    "print(portafoglio)\n",
    "\n",
    "filename_shock = '/Users/lorenzovancadsand/Downloads/shock_results_final(in).csv'\n",
    "shock_data = pd.read_csv(filename_shock)\n",
    "\n",
    "print(shock_data.head())\n",
    "\n",
    "shock_data = shock_data.round(3)\n",
    "\n",
    "print(shock_data.head())\n",
    "\n",
    "max_shock_by_region = shock_data.groupby(['REGION', 'MODEL', 'SCENARIO'], as_index=False).agg({\n",
    "    '2025': 'max',\n",
    "    '2030': 'max',\n",
    "    '2035': 'max',\n",
    "    '2040': 'max',\n",
    "    '2045': 'max',\n",
    "    '2050': 'max'\n",
    "})\n",
    "\n",
    "max_shock_by_region.rename(columns={\n",
    "    '2025': 'Max_Shock_2025',\n",
    "    '2030': 'Max_Shock_2030',\n",
    "    '2035': 'Max_Shock_2035',\n",
    "    '2040': 'Max_Shock_2040',\n",
    "    '2045': 'Max_Shock_2045',\n",
    "    '2050': 'Max_Shock_2050'\n",
    "}, inplace=True)\n",
    "\n",
    "print(max_shock_by_region.head(10))\n",
    "\n",
    "print(shock_data)\n",
    "\n",
    "settori_data = {\n",
    "    'primary_fossil': [\n",
    "        'Primary Energy|Coal',\n",
    "        'Primary Energy|Gas',\n",
    "        'Primary Energy|Oil'\n",
    "    ],\n",
    "    'secondary_fossil': [\n",
    "        'Secondary Energy|Electricity|Coal',\n",
    "        'Secondary Energy|Electricity|Gas',\n",
    "        'Secondary Energy|Electricity|Oil'\n",
    "    ],\n",
    "    'secondary_renewable': [\n",
    "        'Secondary Energy|Electricity|Biomass',\n",
    "        'Secondary Energy|Electricity|Geothermal',\n",
    "        'Secondary Energy|Electricity|Solar|CSP',\n",
    "        'Secondary Energy|Electricity|Nuclear',\n",
    "        'Secondary Energy|Electricity|Solar|PV',\n",
    "        'Secondary Energy|Electricity|Wind',\n",
    "        'Secondary Energy|Electricity|Hydro'\n",
    "    ]\n",
    "}\n",
    "\n",
    "settori_data = {\n",
    "    'primary_fossil': ['Primary Energy|Coal', 'Primary Energy|Gas', 'Primary Energy|Oil'],\n",
    "    'secondary_fossil': ['Secondary Energy|Electricity|Coal', 'Secondary Energy|Electricity|Gas', 'Secondary Energy|Electricity|Oil'],\n",
    "    'secondary_renewable': ['Secondary Energy|Electricity|Biomass', 'Secondary Energy|Electricity|Geothermal',\n",
    "                            'Secondary Energy|Electricity|Solar|CSP', 'Secondary Energy|Electricity|Nuclear',\n",
    "                            'Secondary Energy|Electricity|Solar|PV', 'Secondary Energy|Electricity|Wind', 'Secondary Energy|Electricity|Hydro']\n",
    "}\n",
    "\n",
    "model = 'REMIND'\n",
    "scenario = 'LIMITSOilIndependence'\n",
    "\n",
    "shock_data = shock_data[(shock_data['MODEL'] == model) & (shock_data['SCENARIO'] == scenario)]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num_simulazioni = 10 # number of simulations \n",
    "max_sottosettori_per_settore = 3  # number of sub-sectors randomly associated with each issuer by each category\n",
    "\n",
    "settori_data = {\n",
    "    'primary_fossil': ['Primary Energy|Coal', 'Primary Energy|Gas', 'Primary Energy|Oil'],\n",
    "    'secondary_fossil': ['Secondary Energy|Electricity|Coal', 'Secondary Energy|Electricity|Gas', 'Secondary Energy|Electricity|Oil'],\n",
    "    'secondary_renewable': ['Secondary Energy|Electricity|Biomass', 'Secondary Energy|Electricity|Geothermal',\n",
    "                            'Secondary Energy|Electricity|Solar|CSP', 'Secondary Energy|Electricity|Nuclear',\n",
    "                            'Secondary Energy|Electricity|Solar|PV', 'Secondary Energy|Electricity|Wind', 'Secondary Energy|Electricity|Hydro']\n",
    "}\n",
    "\n",
    "max_percentuali = {\n",
    "    'primary_fossil': 0,\n",
    "    'secondary_fossil': 0,\n",
    "    'secondary_renewable': 100\n",
    "}\n",
    "\n",
    "portafoglio.rename(columns={'Regione': 'REGION'}, inplace=True)\n",
    "\n",
    "lgd_lookup = portafoglio.set_index('Titolo')['LGD_j'].to_dict()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_simulation(sim):\n",
    "    risultati_locali = []  # Lista locale per i risultati\n",
    "    for _, row in portafoglio.iterrows():\n",
    "        titolo = row['Titolo']\n",
    "        regione = row['REGION']\n",
    "        \n",
    "        settori_titolo = [] \n",
    "        \n",
    "        for settore, percentuale in max_percentuali.items():\n",
    "            if percentuale > 0:\n",
    "                available_settori = settori_data[settore]\n",
    "                selezione_settori = random.sample(available_settori, min(max_sottosettori_per_settore, len(available_settori)))\n",
    "                settori_titolo.extend(selezione_settori)\n",
    "        \n",
    "        lgd_j = lgd_lookup.get(titolo, None)\n",
    "        \n",
    "        # Salva i risultati preliminari con LGD_j\n",
    "        for settore in settori_titolo:\n",
    "            risultati_locali.append({\n",
    "                'num_simulazione': sim,\n",
    "                'Titolo': titolo,\n",
    "                'REGION': regione,\n",
    "                'settore': settore,\n",
    "                'LGD_j': lgd_j,\n",
    "            })\n",
    "    return risultati_locali\n",
    "\n",
    "# Parallelizza il ciclo esterno sulle simulazioni\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_simulation)(sim) for sim in tqdm(range(1, num_simulazioni + 1), desc=\"Processing simulations\")\n",
    ")\n",
    "\n",
    "risultati_df = pd.DataFrame([item for sublist in results for item in sublist])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def genera_pesi_interi(gruppo):\n",
    "    \"\"\"\n",
    "    Genera pesi come numeri interi normalizzati con somma esattamente pari a 100.\n",
    "\n",
    "    Parametri: \n",
    "    - gruppo (DataFrame): Gruppo di righe a cui associare i pesi.\n",
    "\n",
    "    Ritorna:\n",
    "    - gruppo (DataFrame): Gruppo con una nuova colonna 'peso' contenente i pesi interi.\n",
    "    \"\"\"\n",
    "    num_righe = len(gruppo)\n",
    "    pesi = np.random.randint(1, 100, size=num_righe)  # Genera numeri interi positivi casuali\n",
    "    \n",
    "    # Normalizza e arrotonda i pesi a interi\n",
    "    pesi = np.round(pesi / pesi.sum() * 100).astype(int)\n",
    "    \n",
    "    # Aggiustamento finale per garantire la somma a 100\n",
    "    differenza = 100 - np.sum(pesi)\n",
    "    pesi[0] += differenza  # Corregge la differenza nel primo elemento\n",
    "    \n",
    "    # Assicura che i pesi siano non negativi\n",
    "    pesi = np.clip(pesi, 0, None)  # Assicura che non ci siano pesi negativi\n",
    "    \n",
    "    gruppo['peso'] = pesi\n",
    "    return gruppo\n",
    "\n",
    "# Applica la funzione di generazione pesi\n",
    "risultati_df = risultati_df.groupby(['num_simulazione', 'Titolo']).apply(genera_pesi_interi).reset_index(drop=True)\n",
    "\n",
    "print(risultati_df)\n",
    "\n",
    "#aggiunta nuovo step\n",
    "\n",
    "from tqdm import tqdm  # Libreria per la progress bar\n",
    "\n",
    "# Definizione degli anni di riferimento\n",
    "anni = list(range(2025, 2055, 5))  # Anni da 2025 a 2050 con step di 5 anni\n",
    "\n",
    "# Creazione di una lista per accumulare le righe della nuova tabella\n",
    "nuova_tabella = []\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Funzione per processare una singola riga di risultati_df\n",
    "def process_row(row, shock_data):\n",
    "    sim = row['num_simulazione']\n",
    "    titolo = row['Titolo']\n",
    "    regione = row['REGION']\n",
    "    settore = row['settore']\n",
    "    peso = row['peso']\n",
    "    lgd_j = row['LGD_j']\n",
    "    \n",
    "    # Filtra shock_data per REGION e VARIABLE corrispondenti\n",
    "    shock_filtro = shock_data[(shock_data['REGION'] == regione) & (shock_data['VARIABLE'] == settore)]\n",
    "    \n",
    "    # Verifica se ci sono dati corrispondenti\n",
    "    if not shock_filtro.empty:\n",
    "        nuova_riga_list = []\n",
    "        for _, shock_row in shock_filtro.iterrows():\n",
    "            modello = shock_row['MODEL']\n",
    "            scenario = shock_row['SCENARIO']\n",
    "            \n",
    "            nuova_riga = {\n",
    "                'num_simulazione': sim,\n",
    "                'Titolo': titolo,\n",
    "                'REGION': regione,\n",
    "                'settore': settore,\n",
    "                'model': modello,\n",
    "                'scenario': scenario,\n",
    "                'peso': peso,\n",
    "                'LGD_j': lgd_j\n",
    "            }\n",
    "            \n",
    "            for anno in range(2025, 2055, 5):\n",
    "                shock_value = shock_row[str(anno)]\n",
    "                nuova_riga[f'shock_{anno}'] = (shock_value * peso) / 100\n",
    "            \n",
    "            nuova_riga_list.append(nuova_riga)\n",
    "        return nuova_riga_list\n",
    "    return []\n",
    "\n",
    "num_cores = -1 \n",
    "results = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "    delayed(process_row)(row, shock_data) for _, row in tqdm(risultati_df.iterrows(), total=len(risultati_df), desc=\"Parallel processing\")\n",
    ")\n",
    "\n",
    "nuova_tabella = [riga for sublist in results for riga in sublist]\n",
    "\n",
    "nuova_tabella_df = pd.DataFrame(nuova_tabella)\n",
    "\n",
    "colonne_ordinate = ['num_simulazione', 'Titolo', 'REGION', 'settore', 'model', 'scenario', 'peso'] + \\\n",
    "                   [f'shock_{anno}' for anno in range(2025, 2055, 5)]\n",
    "\n",
    "nuova_tabella_df = nuova_tabella_df[colonne_ordinate]\n",
    "\n",
    "print(nuova_tabella_df.head())\n",
    "\n",
    "del nuova_tabella\n",
    "del risultati_df\n",
    "gc.collect() \n",
    "\n",
    "\n",
    "tabella_ponderata = nuova_tabella_df.copy()\n",
    "\n",
    "for anno in range(2025, 2055, 5):\n",
    "    colonna_shock = f'shock_{anno}' \n",
    "    colonna_ponderata = f'shock_ponderato_{anno}' \n",
    "    \n",
    "    # Calcola lo shock ponderato\n",
    "    tabella_ponderata[colonna_ponderata] = (tabella_ponderata[colonna_shock] * tabella_ponderata['peso']) / 100\n",
    "\n",
    "colonne_finali = ['num_simulazione', 'Titolo', 'REGION', 'settore', 'model', 'scenario'] + \\\n",
    "                 [f'shock_ponderato_{anno}' for anno in range(2025, 2055, 5)] \n",
    "tabella_ponderata = tabella_ponderata[colonne_finali]\n",
    "\n",
    "print(tabella_ponderata.head())\n",
    "\n",
    "del nuova_tabella_df \n",
    "gc.collect() \n",
    "\n",
    "anni = range(2025, 2055, 5)\n",
    "\n",
    "colonne_shock_ponderato = [f'shock_ponderato_{anno}' for anno in anni]\n",
    "\n",
    "shock_totali = tabella_ponderata.groupby(\n",
    "    ['num_simulazione', 'Titolo', 'model', 'scenario', 'REGION'],\n",
    "    as_index=False\n",
    ")[colonne_shock_ponderato].sum()\n",
    "\n",
    "shock_totali.rename(columns={f'shock_ponderato_{anno}': f'shock_tot_{anno}' for anno in anni}, inplace=True)\n",
    "\n",
    "shock_totali = shock_totali.merge(\n",
    "    max_shock_by_region[['REGION', 'MODEL', 'SCENARIO'] + [f'Max_Shock_{anno}' for anno in anni]],\n",
    "    left_on=['REGION', 'model', 'scenario'],\n",
    "    right_on=['REGION', 'MODEL', 'SCENARIO'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "shock_totali.rename(columns={f'Max_Shock_{anno}': f'max_shock_{anno}' for anno in anni}, inplace=True)\n",
    "\n",
    "shock_totali.drop(columns=['MODEL', 'SCENARIO'], inplace=True)\n",
    "\n",
    "print(shock_totali.head())\n",
    "\n",

    "shock_totali = shock_totali.merge(\n",
    "    portafoglio[['Titolo', 'LGD_j']],  \n",
    "    on='Titolo',                       \n",
    "    how='left'                         \n",
    ")\n",
    "\n",
    "print(shock_totali.head())\n",
    "\n",
    "print(\"Dimensioni di shock_totali dopo il merge:\", shock_totali.shape)\n",
    "\n",
    "del tabella_ponderata \n",
    "gc.collect() \n",
    "\n",
    "anni = range(2025, 2055, 5)\n",
    "\n",
    "risultati_delta_v = []\n",
    "\n",
    "for _, row in shock_totali.iterrows():\n",
    "    nuova_riga = {\n",
    "        'num_simulazione': row['num_simulazione'],\n",
    "        'num_titolo': row['Titolo'],\n",
    "        'model': row['model'],\n",
    "        'scenario': row['scenario']\n",
    "    }\n",
    "    \n",
    "    for anno in anni:\n",
    "        nuova_riga[f'delta_v_{anno}'] = - (row['LGD_j'] * row[f'shock_tot_{anno}']) / (row[f'max_shock_{anno}'] + 1)\n",
    "    \n",
    "    risultati_delta_v.append(nuova_riga)\n",
    "\n",
    "delta_v_df = pd.DataFrame(risultati_delta_v)\n",
    "\n",
    "print(delta_v_df.head())\n",
    "\n",
    "anni = range(2025, 2055, 5)\n",
    "delta_v_cols = [f'delta_v_{anno}' for anno in anni]  # Colonne da sommare\n",
    "\n",
    "aggregated_losses = delta_v_df.groupby(['num_simulazione', 'model', 'scenario'], as_index=False)[delta_v_cols].sum()\n",
    "\n",
    "rename_dict = {f'delta_v_{anno}': f'sum_delta_v_{anno}' for anno in anni}\n",
    "aggregated_losses.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "print(aggregated_losses.head())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "anni = range(2025, 2055, 5)\n",
    "loss_cols = [f'sum_delta_v_{anno}' for anno in anni] \n",
    "\n",
    "percentile_results_list = []\n",
    "\n",
    "for anno, loss_col in zip(anni, loss_cols):\n",
    "    percentile_col = f'percentile_99.5_{anno}' \n",
    "    \n",
    "    if loss_col in aggregated_losses.columns:\n",
    "        temp_percentile = aggregated_losses.groupby(['model', 'scenario'], as_index=False)[loss_col].quantile(0.995)\n",
    "        \n",
    "        temp_percentile.rename(columns={loss_col: percentile_col}, inplace=True)\n",
    "        \n",
    "        percentile_results_list.append(temp_percentile)\n",
    "    else:\n",
    "        print(f\"Colonna {loss_col} non trovata in aggregated_losses.\")\n",
    "\n",
    "percentile_results = percentile_results_list[0]\n",
    "for i in range(1, len(percentile_results_list)):\n",
    "    percentile_results = pd.merge(percentile_results, percentile_results_list[i], on=['model', 'scenario'], how='inner')\n",
    "\n",
    "print(percentile_results)\n",
    "\n",
    "anni = range(2025, 2055, 5)\n",
    "y_f = 0.03 \n",
    "\n",
    "adjusted_table = percentile_results.copy()\n",
    "\n",
    "for anno in anni:\n",
    "    T = anno - 2025 \n",
    "    discount = np.exp(-y_f * T) \n",
    "    \n",
    "    percentile_col = f'percentile_99.5_{anno}'\n",
    "    adjusted_col = f'adjusted_{anno}'\n",
    "    \n",
    "    if percentile_col in adjusted_table.columns:\n",
    "        adjusted_table[adjusted_col] = adjusted_table[percentile_col] * discount\n",
    "    else:\n",
    "        print(f\"Colonna {percentile_col} non trovata in adjusted_table.\")\n",
    "\n",
    "columns_to_remove = [f'percentile_99.5_{anno}' for anno in anni]\n",
    "adjusted_table.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
    "n\n",
    "print(adjusted_table.head())\n",
    "\n",
    "output_path = '/users/lorenzovancadsand/downloads/Basis_Africa_ROIND.xlsx'\n",
    "adjusted_table.to_excel(output_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tempo totale di esecuzione: {end_time - start_time:.2f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7608f9c-23be-4617-b3be-606e2fff2c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
