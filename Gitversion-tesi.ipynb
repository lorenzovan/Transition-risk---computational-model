{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef29e8d-ba8d-4ac4-8e81-706b8faaec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import dask\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mean = 1\n",
    "variance = 4\n",
    "k = mean ** 2 / variance\n",
    "theta = variance / mean\n",
    "\n",
    "random_values = gamma.rvs(k, scale=theta, size=1000000)\n",
    "\n",
    "percentile_99_5 = np.percentile(random_values, 99.5)\n",
    "\n",
    "rating = ['A', 'BBB', 'BB', 'B', 'CCC']\n",
    "p = np.array([0.06, 0.20, 1.25, 6.25, 17.50])\n",
    "w = np.array([1.011, 0.836, 0.602, 0.415, 0.295])\n",
    "\n",
    "rating_table = pd.DataFrame({\n",
    "    'Rating': rating,\n",
    "    'p_mean': p,\n",
    "    'w': w\n",
    "})\n",
    "\n",
    "print('Table of associated values:')\n",
    "print(rating_table)\n",
    "\n",
    "E_LGD = 0.50\n",
    "\n",
    "rating_table['mu_x'] = E_LGD * rating_table['p_mean'] * (1 + rating_table['w'] * (percentile_99_5 - 1))\n",
    "\n",
    "print('Table with mu_x values:')\n",
    "print(rating_table)\n",
    "\n",
    "sigma = 2\n",
    "lambda_ = 0.5\n",
    "eta = 0.25\n",
    "sigma2 = sigma ** 2\n",
    "\n",
    "rating_table['Beta'] = (1 / (2 * lambda_)) * (lambda_ ** 2 + eta ** 2) * (\n",
    "        (1 / sigma2) * (1 + (sigma2 - 1) / percentile_99_5) *\n",
    "        (percentile_99_5 + (1 - rating_table['w']) / rating_table['w']) - 1)\n",
    "\n",
    "n_values = [5000, 2000, 1000, 500, 200, 100]\n",
    "column_names = ['VaR5000', 'VaR2000', 'VaR1000', 'VaR500', 'VaR200', 'VaR100']\n",
    "\n",
    "for i, n in enumerate(n_values):\n",
    "    rating_table[column_names[i]] = rating_table['mu_x'] + (rating_table['Beta'] / n) * 100\n",
    "\n",
    "rating_table = rating_table.drop(columns=['Beta'])\n",
    "\n",
    "rating_table.to_excel('/users/lorenzovancadsand/downloads/granularity_add-on.xlsx', index=False)\n",
    "\n",
    "dask_data = da.from_array(random_values, chunks=100000)\n",
    "\n",
    "@delayed\n",
    "def compute_percentile(data):\n",
    "    return np.percentile(data, 99.5)\n",
    "\n",
    "result = compute_percentile(dask_data)\n",
    "\n",
    "percentile_99_5_parallel = result.compute()\n",
    "print(f'99Â° Percentile: {percentile_99_5_parallel}')\n",
    "\n",
    "\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "file_path = '/Users/lorenzovancadsand/Downloads/portafoglio_Asia.csv'\n",
    "portfolio = pd.read_csv(file_path)\n",
    "\n",
    "print('Portfolio simulations:')\n",
    "print(portfolio)\n",
    "\n",
    "lambda_ = 0.5\n",
    "eta = 0.25\n",
    "\n",
    "num_bonds = len(portfolio)\n",
    "\n",
    "LGD_j = np.random.gamma(shape=lambda_, scale=eta, size=num_bonds)\n",
    "\n",
    "portfolio['LGD_j'] = LGD_j\n",
    "portfolio = portfolio.round(3)\n",
    "\n",
    "regions = ['NORTH_AM', 'EUROPE', 'ASIA', 'AFRICA', 'LATIN_AM', 'REST_WORLD']\n",
    "associated_regions = portfolio['Regione']\n",
    "\n",
    "years = [2025, 2030, 2035, 2040, 2045, 2050]\n",
    "\n",
    "print('Updated portfolio with LGD_j:')\n",
    "print(portfolio)\n",
    "\n",
    "filename_shock = '/Users/lorenzovancadsand/Downloads/Shock_Asia(Foglio1).csv'\n",
    "shock_data = pd.read_csv(filename_shock)\n",
    "\n",
    "print(shock_data.head())\n",
    "\n",
    "shock_data = shock_data.round(3)\n",
    "\n",
    "print(shock_data.head())\n",
    "\n",
    "# Group and calculate the maximum shock for each combination of REGION, MODEL, SCENARIO\n",
    "max_shock_by_region = shock_data.groupby(['REGION', 'MODEL', 'SCENARIO'], as_index=False).agg({\n",
    "    '2025': 'max',\n",
    "    '2030': 'max',\n",
    "    '2035': 'max',\n",
    "    '2040': 'max',\n",
    "    '2045': 'max',\n",
    "    '2050': 'max'\n",
    "})\n",
    "\n",
    "max_shock_by_region.rename(columns={\n",
    "    '2025': 'Max_Shock_2025',\n",
    "    '2030': 'Max_Shock_2030',\n",
    "    '2035': 'Max_Shock_2035',\n",
    "    '2040': 'Max_Shock_2040',\n",
    "    '2045': 'Max_Shock_2045',\n",
    "    '2050': 'Max_Shock_2050'\n",
    "}, inplace=True)\n",
    "\n",
    "print(max_shock_by_region.head(10))\n",
    "\n",
    "print(shock_data)\n",
    "\n",
    "sectors_data = {\n",
    "    'primary_fossil': [\n",
    "        'Primary Energy|Coal',\n",
    "        'Primary Energy|Gas',\n",
    "        'Primary Energy|Oil'\n",
    "    ],\n",
    "    'secondary_fossil': [\n",
    "        'Secondary Energy|Electricity|Coal',\n",
    "        'Secondary Energy|Electricity|Gas',\n",
    "        'Secondary Energy|Electricity|Oil'\n",
    "    ],\n",
    "    'secondary_renewable': [\n",
    "        'Secondary Energy|Electricity|Biomass',\n",
    "        'Secondary Energy|Electricity|Geothermal',\n",
    "        'Secondary Energy|Electricity|Solar|CSP',\n",
    "        'Secondary Energy|Electricity|Nuclear',\n",
    "        'Secondary Energy|Electricity|Solar|PV',\n",
    "        'Secondary Energy|Electricity|Wind',\n",
    "        'Secondary Energy|Electricity|Hydro'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Primary Fossil Sectors:\")\n",
    "print(sectors_data['primary_fossil'])\n",
    "\n",
    "print(\"\\nSecondary Renewable Sectors:\")\n",
    "print(sectors_data['secondary_renewable'])\n",
    "\n",
    "model = 'REMIND'\n",
    "scenario = 'LIMITSOilIndependence'\n",
    "\n",
    "shock_data = shock_data[(shock_data['MODEL'] == model) & (shock_data['SCENARIO'] == scenario)]\n",
    "\n",
    "\n",
    "\n",
    "num_simulations = 1000\n",
    "max_subsectors_per_sector = 3\n",
    "\n",
    "sectors_data = {\n",
    "    'primary_fossil': ['Primary Energy|Coal', 'Primary Energy|Gas', 'Primary Energy|Oil'],\n",
    "    'secondary_fossil': ['Secondary Energy|Electricity|Coal', 'Secondary Energy|Electricity|Gas', 'Secondary Energy|Electricity|Oil'],\n",
    "    'secondary_renewable': ['Secondary Energy|Electricity|Biomass', 'Secondary Energy|Electricity|Geothermal',\n",
    "                            'Secondary Energy|Electricity|Solar|CSP', 'Secondary Energy|Electricity|Nuclear',\n",
    "                            'Secondary Energy|Electricity|Solar|PV', 'Secondary Energy|Electricity|Wind', 'Secondary Energy|Electricity|Hydro']\n",
    "}\n",
    "\n",
    "max_percentages = {\n",
    "    'primary_fossil': 0,\n",
    "    'secondary_fossil': 0,\n",
    "    'secondary_renewable': 100\n",
    "}\n",
    "\n",
    "portfolio.rename(columns={'Regione': 'REGION'}, inplace=True)\n",
    "\n",
    "lgd_lookup = portfolio.set_index('Titolo')['LGD_j'].to_dict()\n",
    "\n",
    "\n",
    "def process_simulation(sim):\n",
    "    local_results = []\n",
    "    for _, row in portfolio.iterrows():\n",
    "        bond = row['Titolo']\n",
    "        region = row['REGION']\n",
    "\n",
    "        bond_sectors = []\n",
    "\n",
    "        for sector, percentage in max_percentages.items():\n",
    "            if percentage > 0:\n",
    "                available_sectors = sectors_data[sector]\n",
    "                selected_subsectors = random.sample(available_sectors, min(max_subsectors_per_sector, len(available_sectors)))\n",
    "                bond_sectors.extend(selected_subsectors)\n",
    "\n",
    "        lgd_j = lgd_lookup.get(bond, None)\n",
    "\n",
    "        for sector in bond_sectors:\n",
    "            local_results.append({\n",
    "                'simulation_num': sim,\n",
    "                'Titolo': bond,\n",
    "                'REGION': region,\n",
    "                'sector': sector,\n",
    "                'LGD_j': lgd_j,\n",
    "            })\n",
    "    return local_results\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_simulation)(sim) for sim in tqdm(range(1, num_simulations + 1), desc=\"Processing simulations\")\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame([item for sublist in results for item in sublist])\n",
    "\n",
    "\n",
    "\n",
    "def generate_integer_weights(group):\n",
    "    \"\"\"\n",
    "    Generates weights as normalized integers with a sum exactly equal to 100.\n",
    "\n",
    "    Parameters:\n",
    "    - group (DataFrame): Group of rows to associate the weights with.\n",
    "\n",
    "    Returns:\n",
    "    - group (DataFrame): Group with a new 'weight' column containing the integer weights.\n",
    "    \"\"\"\n",
    "    num_rows = len(group)\n",
    "    weights = np.random.randint(1, 100, size=num_rows)\n",
    "\n",
    "    weights = np.round(weights / weights.sum() * 100).astype(int)\n",
    "\n",
    "    difference = 100 - np.sum(weights)\n",
    "    weights[0] += difference  # Corrects the difference in the first element\n",
    "\n",
    "    weights = np.clip(weights, 0, None)\n",
    "\n",
    "    group['weight'] = weights\n",
    "    return group\n",
    "\n",
    "results_df = results_df.groupby(['simulation_num', 'Titolo']).apply(generate_integer_weights).reset_index(drop=True)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "years = list(range(2025, 2055, 5))\n",
    "\n",
    "new_table = []\n",
    "\n",
    "def process_row(row, shock_data):\n",
    "    sim = row['simulation_num']\n",
    "    bond = row['Titolo']  \n",
    "    region = row['REGION']\n",
    "    sector = row['sector']\n",
    "    weight = row['weight']\n",
    "    lgd_j = row['LGD_j']\n",
    "\n",
    "    shock_filter = shock_data[(shock_data['REGION'] == region) & (shock_data['VARIABLE'] == sector)]\n",
    "\n",
    "    if not shock_filter.empty:\n",
    "        new_row_list = []\n",
    "        for _, shock_row in shock_filter.iterrows():\n",
    "            model = shock_row['MODEL']\n",
    "            scenario = shock_row['SCENARIO']\n",
    "\n",
    "            new_row = {\n",
    "                'simulation_num': sim,\n",
    "                'Titolo': bond,\n",
    "                'REGION': region,\n",
    "                'sector': sector,\n",
    "                'model': model,\n",
    "                'scenario': scenario,\n",
    "                'weight': weight,\n",
    "                'LGD_j': lgd_j\n",
    "            }\n",
    "\n",
    "            for year in range(2025, 2055, 5):\n",
    "                shock_value = shock_row[str(year)]\n",
    "                new_row[f'shock_{year}'] = (shock_value * weight) / 100\n",
    "\n",
    "            new_row_list.append(new_row)\n",
    "        return new_row_list\n",
    "    return []\n",
    "\n",
    "num_cores = -1  \n",
    "results = Parallel(n_jobs=num_cores, backend=\"loky\")(\n",
    "    delayed(process_row)(row, shock_data) for _, row in tqdm(results_df.iterrows(), total=len(results_df), desc=\"Parallel processing\")\n",
    ")\n",
    "\n",
    "new_table = [row for sublist in results for row in sublist]\n",
    "\n",
    "new_table_df = pd.DataFrame(new_table)\n",
    "\n",
    "ordered_columns = ['simulation_num', 'Titolo', 'REGION', 'sector', 'model', 'scenario', 'weight'] + \\\n",
    "                      [f'shock_{year}' for year in range(2025, 2055, 5)]\n",
    "\n",
    "new_table_df = new_table_df[ordered_columns]\n",
    "\n",
    "print(new_table_df.head())\n",
    "\n",
    "\n",
    "weighted_table = new_table_df.copy()\n",
    "\n",
    "for year in range(2025, 2055, 5):\n",
    "    shock_column = f'shock_{year}'  \n",
    "    weighted_column = f'weighted_shock_{year}'  \n",
    "\n",
    "    weighted_table[weighted_column] = (weighted_table[shock_column] * weighted_table['weight']) / 100\n",
    "\n",
    "final_columns = ['simulation_num', 'Titolo', 'REGION', 'sector', 'model', 'scenario'] + \\\n",
    "                  [f'weighted_shock_{year}' for year in range(2025, 2055, 5)]\n",
    "weighted_table = weighted_table[final_columns]\n",
    "\n",
    "print(weighted_table.head())\n",
    "\n",
    "del new_table_df\n",
    "gc.collect()\n",
    "\n",
    "years = range(2025, 2055, 5)\n",
    "\n",
    "weighted_shock_columns = [f'weighted_shock_{year}' for year in years]\n",
    "\n",
    "total_shocks = weighted_table.groupby(\n",
    "    ['simulation_num', 'Titolo', 'model', 'scenario', 'REGION'],\n",
    "    as_index=False\n",
    ")[weighted_shock_columns].sum()\n",
    "\n",
    "total_shocks.rename(columns={f'weighted_shock_{year}': f'shock_tot_{year}' for year in years}, inplace=True)\n",
    "\n",
    "total_shocks = total_shocks.merge(\n",
    "    max_shock_by_region[['REGION', 'MODEL', 'SCENARIO'] + [f'Max_Shock_{year}' for year in years]],\n",
    "    left_on=['REGION', 'model', 'scenario'],\n",
    "    right_on=['REGION', 'MODEL', 'SCENARIO'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "total_shocks.rename(columns={f'Max_Shock_{year}': f'max_shock_{year}' for year in years}, inplace=True)\n",
    "\n",
    "total_shocks.drop(columns=['MODEL', 'SCENARIO'], inplace=True)\n",
    "\n",
    "print(total_shocks.head())\n",
    "\n",
    "total_shocks = total_shocks.merge(\n",
    "    portfolio[['Titolo', 'LGD_j']],  # Take only Titolo and LGD_j from portfolio\n",
    "    on='Titolo',                       # Merge key\n",
    "    how='left'                         # Keep all rows of total_shocks\n",
    ")\n",
    "\n",
    "print(total_shocks.head())\n",
    "\n",
    "print(\"Dimensions of total_shocks after merge:\", total_shocks.shape)\n",
    "\n",
    "del weighted_table\n",
    "gc.collect()\n",
    "\n",
    "years = range(2025, 2055, 5)\n",
    "\n",
    "delta_v_results = []\n",
    "\n",
    "for _, row in total_shocks.iterrows():\n",
    "    new_row = {\n",
    "        'simulation_num': row['simulation_num'],\n",
    "        'bond_num': row['Titolo'],\n",
    "        'model': row['model'],\n",
    "        'scenario': row['scenario']\n",
    "    }\n",
    "\n",
    "    for year in years:\n",
    "        new_row[f'delta_v_{year}'] = - (row['LGD_j'] * row[f'shock_tot_{year}']) / (row[f'max_shock_{year}'] + 1)\n",
    "\n",
    "    delta_v_results.append(new_row)\n",
    "\n",
    "delta_v_df = pd.DataFrame(delta_v_results)\n",
    "\n",
    "print(delta_v_df.head())\n",
    "\n",
    "years = range(2025, 2055, 5)\n",
    "delta_v_cols = [f'delta_v_{year}' for year in years] \n",
    "\n",
    "aggregated_losses = delta_v_df.groupby(['simulation_num', 'model', 'scenario'], as_index=False)[delta_v_cols].sum()\n",
    "\n",
    "rename_dict = {f'delta_v_{year}': f'sum_delta_v_{year}' for year in years}\n",
    "aggregated_losses.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "print(aggregated_losses.head())\n",
    "\n",
    "years = range(2025, 2055, 5)\n",
    "loss_cols = [f'sum_delta_v_{year}' for year in years]  # Loss columns\n",
    "\n",
    "percentile_results_list = []\n",
    "\n",
    "for year, loss_col in zip(years, loss_cols):\n",
    "    percentile_col = f'percentile_99.5_{year}'\n",
    "\n",
    "    if loss_col in aggregated_losses.columns:\n",
    "        temp_percentile = aggregated_losses.groupby(['model', 'scenario'], as_index=False)[loss_col].quantile(0.995)\n",
    "\n",
    "        temp_percentile.rename(columns={loss_col: percentile_col}, inplace=True)\n",
    "\n",
    "        percentile_results_list.append(temp_percentile)\n",
    "    else:\n",
    "        print(f\"Column {loss_col} not found in aggregated_losses.\")\n",
    "\n",
    "percentile_results = percentile_results_list[0]\n",
    "for i in range(1, len(percentile_results_list)):\n",
    "    percentile_results = pd.merge(percentile_results, percentile_results_list[i], on=['model', 'scenario'], how='inner')\n",
    "\n",
    "print(percentile_results)\n",
    "\n",
    "years = range(2025, 2055, 5)\n",
    "risk_free_rate = 0.03  \n",
    "\n",
    "adjusted_table = percentile_results.copy()\n",
    "\n",
    "for year in years:\n",
    "    T = year - 2025 \n",
    "    discount = np.exp(-risk_free_rate * T)  \n",
    "\n",
    "    percentile_col = f'percentile_99.5_{year}'\n",
    "    adjusted_col = f'adjusted_{year}'\n",
    "\n",
    "    if percentile_col in adjusted_table.columns:\n",
    "        adjusted_table[adjusted_col] = adjusted_table[percentile_col] * discount\n",
    "    else:\n",
    "        print(f\"Column {percentile_col} not found in adjusted_table.\")\n",
    "\n",
    "columns_to_remove = [f'percentile_99.5_{year}' for year in years]\n",
    "adjusted_table.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "print(adjusted_table.head())\n",
    "\n",
    "output_path = '/users/lorenzovancadsand/downloads/Basis_Africa_ROIND.xlsx'\n",
    "adjusted_table.to_excel(output_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7dcda-6ce7-471e-9db7-57f484805be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
